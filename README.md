## Files in Repository
File	Description
week5_transformers.ipynb	Main notebook for both assignments
)
requirements.txt	Dependencies list
data/	Optional folder for custom fine-tuning dataset
outputs/	Model checkpoints and evaluation results
ğŸ“ˆ Key Learnings

Hugging Face simplifies fine tuning of large NLP models.

Transfer learning drastically reduces the required data and compute.

Domain-specific fine-tuning enhances translation accuracy.

Evaluation metrics (Accuracy, BLEU) are essential for model validation.

ğŸ’¡ Future Work

Expand fine-tuning dataset to multiple domains (finance, legal, etc.).

Deploy the fine tuned model via an API endpoint (e.g., FastAPI + Gradio).

Experiment with multilingual models like mBERT or NLLB for cross-language tasks.

ğŸ‘¨ğŸ½â€ğŸ’» Author

Olumide Buari

ğŸ“§ buariolumide@gmail.com

ğŸ”— LinkedIn: https://www.linkedin.com/olumide-b-adekunle

ğŸ’» GitHub: https://www.github.com/olumideadekunle

# Transformers for Text Classification and Machine Translation â€” Applied Learning
These experiments demonstrate how transfer learning enables efficient adaptation of large language models (LLMs) for specific NLP applications.
